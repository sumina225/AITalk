import logging
import pyaudio
import numpy as np
import time
import openai
from threading import Lock, Thread
import os
from gtts import gTTS
import base64
from io import BytesIO
import wave
import tempfile
import json
import requests
from pydub import AudioSegment  # ğŸ”¥ ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”
from flask import jsonify

from app.extensions import socketio, db
from app.models import Child, Schedule
from dotenv import load_dotenv
from sqlalchemy.orm.attributes import flag_modified

load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# ìŒì„± ì¸ì‹ ë° TTS, GPT ì²˜ë¦¬ ìƒíƒœ ê´€ë¦¬ ì „ì—­ ë³€ìˆ˜
is_recognizing = False
recognition_lock = Lock()
keep_listening = True
gpt_processing = False
is_tts_playing = True
current_child_id = None

# OpenAI API ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")
TYPECAST_API_KEY = os.getenv("TYPECAST_API_KEY")
TYPECAST_ACTOR_ID = os.getenv("TYPECAST_VOICE_ID")

# ëŒ€í™” ë‚´ì—­ (ì´ˆê¸° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨)
conversation_history = []


def get_child_info(child_id):
    child = Child.query.filter_by(child_id=child_id).first()
    if child:
        return {
            'child_name': child.child_name,
            'child_age': child.age,
            'disability_type': child.disability_type
        }
    return None

def text_to_speech(text):
    """
    Typecast APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•œ í›„, Base64 MP3ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜.
    Polling ë°©ì‹ì„ ì´ìš©í•˜ì—¬ ìŒì„± í•©ì„±ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°í•œ í›„, ê²°ê³¼ë¥¼ ë‹¤ìš´ë¡œë“œí•œë‹¤.
    """
    if not TYPECAST_API_KEY or not TYPECAST_ACTOR_ID:
        logging.error("âŒ Typecast API Key ë˜ëŠ” Actor IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None

    api_url = "https://typecast.ai/api/speak"
    headers = {
        "Authorization": f"Bearer {TYPECAST_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = json.dumps({
        "actor_id": TYPECAST_ACTOR_ID,
        "text": text,
        "lang": "auto",
        "xapi_audio_format": "mp3"
    })

    try:
        # âœ… Typecast API ìš”ì²­ (ìŒì„± ìƒì„±)
        response = requests.post(api_url, headers=headers, data=payload)
        if response.status_code != 200:
            logging.error(f"âŒ Typecast API ì˜¤ë¥˜: {response.status_code} - {response.text}")
            return None

        # âœ… ì‘ë‹µì—ì„œ Polling URL ì¶”ì¶œ
        result = response.json()
        speak_v2_url = result["result"]["speak_v2_url"]
        logging.info(f"ğŸ“¢ Polling ì‹œì‘... URL: {speak_v2_url}")

        # âœ… Polling (ìµœëŒ€ 60ì´ˆ ë™ì•ˆ ëŒ€ê¸°)
        for _ in range(60):
            time.sleep(1)  # 1ì´ˆ ê°„ê²©ìœ¼ë¡œ ìƒíƒœ í™•ì¸
            poll_response = requests.get(speak_v2_url, headers=headers)
            poll_result = poll_response.json()["result"]

            if poll_result["status"] == "done":
                audio_download_url = poll_result["audio_download_url"]
                logging.info(f"ğŸ‰ ìŒì„± í•©ì„± ì™„ë£Œ! ë‹¤ìš´ë¡œë“œ URL: {audio_download_url}")
                break
            else:
                logging.info(f"âŒ› ìŒì„± ì²˜ë¦¬ ì¤‘... (í˜„ì¬ ìƒíƒœ: {poll_result['status']})")
        else:
            logging.error("âŒ ìŒì„± í•©ì„± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return None

        # âœ… ì˜¤ë””ì˜¤ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        audio_response = requests.get(audio_download_url)
        if audio_response.status_code != 200:
            logging.error(f"âŒ ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {audio_response.status_code}")
            return None

        # âœ… Base64ë¡œ ë³€í™˜ í›„ ë°˜í™˜
        audio_base64 = base64.b64encode(audio_response.content).decode('utf-8')
        return audio_base64

    except Exception as e:
        logging.error(f"âŒ Typecast API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None



def initialize_conversation(child_id):
    child_info = get_child_info(child_id)
    if not child_info:
        return

    initial_system_prompt = f"""
    You are a language therapy chatbot designed for a child who is {child_info['child_age']} years old and has {child_info['disability_type']}.  
    Your goal is to encourage the child to speak more by asking **simple, closed-ended questions** (yes/no questions or offering simple choices), but do **not** instruct the child to answer only with "yes" or "no." Allow natural, flexible responses from the child.  

    Since the child may have speech difficulties and might not pronounce words clearly, try to understand the meaning based on **context** even if the words sound incorrect.  
    If the child's response is similar to one of the given choices but slightly incorrect (e.g., "ë£¸ë¨¸ë¦¬" instead of "ë¸”ë¡ë†€ì´," or "ê·¸ë¦¬ë¯¸" instead of "ê·¸ë¦¼ê·¸ë¦¬ê¸°"), interpret it as the closest intended word.  
    However, if the response is completely unrelated (e.g., "ë°¥ ë¨¹ì—ˆì–´ìš”"), acknowledge the response but gently guide the child to choose between the given options.  

    Keep the questions simple and age-appropriate. Make sure to respond **in Korean**.  
    If the child successfully chooses an option, provide a follow-up question or encouragement to continue the conversation.  

    """
    logging.info(f"ì´ˆê¸° í”„ë¡¬í”„íŠ¸: {initial_system_prompt}")
    conversation_history.clear()
    conversation_history.append({"role": "system", "content": initial_system_prompt})



def get_input_device():
    """ EarPods ë˜ëŠ” Logitech StreamCam ì¤‘ ìš°ì„  ì„ íƒ ê°€ëŠ¥í•œ ë§ˆì´í¬ë¥¼ ë°˜í™˜ """
    p = pyaudio.PyAudio()
    input_device_index = None
    preferred_device_index = None  # ìµœì ì˜ ì¥ì¹˜ë¥¼ ì €ì¥í•˜ëŠ” ë³€ìˆ˜

    for i in range(p.get_device_count()):
        device_info = p.get_device_info_by_index(i)
        device_name = device_info["name"]
        max_channels = device_info["maxInputChannels"]

        logging.info(f"ğŸ¤ Device {i}: {device_name} - Input Channels: {max_channels}")

        # ğŸ¯ Logitech StreamCamì´ ê°ì§€ë˜ë©´ ìµœìš°ì„  ì„ íƒ (ì´ë¦„ í¬í•¨ ì—¬ë¶€ ì²´í¬)
        if "Logitech StreamCam" in device_name and max_channels > 0:
            preferred_device_index = i  # Logitech StreamCamì„ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒ
            break  # StreamCamì„ ì°¾ì•˜ìœ¼ë©´ ì¦‰ì‹œ ì¢…ë£Œ

        # ğŸ¯ EarPodsì´ ê°ì§€ë˜ë©´ ì„ íƒ í›„ë³´ë¡œ ì €ì¥ (ì´ë¦„ í¬í•¨ ì—¬ë¶€ ì²´í¬)
        if "EarPods" in device_name and max_channels > 0:
            input_device_index = i

    p.terminate()

    # âœ… Logitech StreamCamì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì—†ìœ¼ë©´ EarPods ì‚¬ìš©
    final_device = preferred_device_index if preferred_device_index is not None else input_device_index

    if final_device is None:
        logging.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë§ˆì´í¬ ì…ë ¥ ì¥ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        logging.info(f"âœ… ì„ íƒëœ ì…ë ¥ ì¥ì¹˜: Device {final_device}")

    return final_device


def recognize_audio(child_id):
    global is_recognizing, keep_listening, is_tts_playing

    with recognition_lock:
        is_recognizing = True

    CHUNK = 1024
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 48000
    RECORD_SECONDS = 2.0  

    # ğŸ”¥ ì…ë ¥ ì¥ì¹˜ ì°¾ê¸°
    input_device_index = get_input_device()
    if input_device_index is None:
        logging.error("ğŸš¨ ë§ˆì´í¬ ì…ë ¥ ì¥ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤. ìŒì„± ì¸ì‹ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    logging.info(f"ğŸ™ ì„ íƒëœ ë§ˆì´í¬ ì¥ì¹˜ ì¸ë±ìŠ¤: {input_device_index}")

    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True,
                    frames_per_buffer=CHUNK, input_device_index=input_device_index)

    logging.info("ğŸ™ ìŒì„± ì¸ì‹ ì‹œì‘")
    socketio.emit("speech_ready")

    audio_buffer = []
    silence_threshold = 0.01
    silence_duration = 1.0  

    last_speech_time = time.time()

    while keep_listening:
        if is_tts_playing:
            if audio_buffer:
                logging.debug("ğŸ”‡ TTS ì¬ìƒ ì¤‘: audio_buffer ì´ˆê¸°í™”")
                audio_buffer.clear()
            time.sleep(0.1)
            continue

        frames = [stream.read(CHUNK, exception_on_overflow=False)
                  for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS))]

        audio_data = b''.join(frames)
        audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0

        # ğŸ”¥ RMS ê³„ì‚° (ìŒì„± ê°ì§€)
        rms = np.sqrt(np.mean(np.square(audio_np)))
        logging.debug(f"ğŸ”Š RMS ê°’: {rms:.4f}, ë²„í¼ ê¸¸ì´: {len(audio_buffer)}")

        if rms > silence_threshold:
            logging.debug("ğŸ¤ ìŒì„± ê°ì§€ë¨")
            socketio.emit('speech_detected', {'status': 'speaking'}, namespace='/')
            audio_buffer.append(audio_data)
            last_speech_time = time.time()
        else:
            if len(audio_buffer) > 0 and (time.time() - last_speech_time > silence_duration):
                logging.info("ğŸ” ë§ ë©ˆì¶¤ ê°ì§€ â†’ ì¦‰ì‹œ í…ìŠ¤íŠ¸ ë³€í™˜ ì‹œì‘")
                socketio.emit('speech_stopped', {'status': 'silent'}, namespace='/')

                is_tts_playing = True
                full_audio = b''.join(audio_buffer)
                audio_buffer.clear()

                # ğŸ”¥ Whisper API ë³€í™˜ ì‹œë„
                try:
                    wav_io = BytesIO()
                    with wave.open(wav_io, "wb") as wf:
                        wf.setnchannels(CHANNELS)
                        wf.setsampwidth(p.get_sample_size(FORMAT))
                        wf.setframerate(RATE)
                        wf.writeframes(full_audio)
                    wav_io.seek(0)

                    # ğŸ”¥ ì„ì‹œ WAV íŒŒì¼ ì €ì¥ í›„ OpenAI Whisper ì‚¬ìš©
                    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
                        tmp.write(wav_io.getvalue())
                        tmp_path = tmp.name

                    with open(tmp_path, "rb") as audio_file:
                        result = openai.audio.transcriptions.create(
                            model="whisper-1",
                            file=audio_file,
                            language="ko",
                            temperature=0.2
                        )
                    text = result.text.strip()

                    if text:
                        logging.info(f"ğŸ“ ë³€í™˜ëœ í…ìŠ¤íŠ¸: {text}")
                        Thread(target=get_gpt_response, args=(text, child_id), daemon=True).start()
                    else:
                        logging.warning("âš ï¸ ìŒì„±ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í•¨")
                        is_tts_playing = False

                except Exception as e:
                    logging.error(f"âŒ í…ìŠ¤íŠ¸ ë³€í™˜ ì˜¤ë¥˜: {e}")
                    is_tts_playing = False

                finally:
                    # ğŸ”¥ ì„ì‹œ íŒŒì¼ ì‚­ì œ
                    if os.path.exists(tmp_path):
                        os.remove(tmp_path)

            else:
                logging.debug("ğŸ¤« ì¹¨ë¬µ ê°ì§€ ì¤‘...")

        time.sleep(0.01)

    stream.stop_stream()
    stream.close()
    p.terminate()
    logging.info("ğŸ™ ìŒì„± ì¸ì‹ ì¢…ë£Œ")
    with recognition_lock:
        is_recognizing = False






def get_gpt_response(user_input, child_id, is_summary=False):
    global gpt_processing, conversation_history, is_tts_playing
    try:
        conversation_history.append({"role": "user", "content": user_input})

        # ëŒ€í™” ì´ë ¥ 10ê°œê¹Œì§€ë§Œ ìœ ì§€
        if len(conversation_history) > 10:
            conversation_history = conversation_history[-10:]

        logging.info(f"ğŸ¤ GPTì— ì§ˆë¬¸: {user_input}")
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=conversation_history,
            max_tokens=500
        )

        gpt_reply = response.choices[0].message.content.strip()
        logging.info(f"ğŸ¤– GPT ì‘ë‹µ: {gpt_reply}")
        conversation_history.append({"role": "assistant", "content": gpt_reply})

        if not is_summary:
            is_tts_playing = True
            audio_base64 = text_to_speech(gpt_reply)
            socketio.emit('gpt_response', {'response': gpt_reply, 'audio': audio_base64}, namespace='/')
        else:
            logging.info("âœ… ìš”ì•½ ì™„ë£Œ (í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡í•˜ì§€ ì•ŠìŒ)")
            return gpt_reply

    except Exception as e:
        logging.error(f"âŒ GPT ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        gpt_processing = False
        socketio.emit('gpt_ready', {'status': 'ready'}, namespace='/')


def stop_recognition(child_id, schedule_id=None):
    global keep_listening, conversation_history
    keep_listening = False
    logging.info("ğŸ” ìŒì„± ì¸ì‹ ì¤‘ë‹¨")

    child_info = get_child_info(child_id)
    if child_info:
        summary_prompt = f"""
                You are a speech therapist reviewing a conversation with a child who is {child_info['child_age']} years old and has {child_info['disability_type']}.
                Please summarize the conversation from a speech therapistâ€™s perspective in **3 concise sentences**. Focus on the childâ€™s response style, speech and pronunciation, vocabulary use, and engagement, but do not list these points separately.
                Provide the summary in **Korean**, and keep the summary within **100 characters**.
                """

        # ìš”ì•½ ìƒì„± ë° ì €ì¥
        summary = get_gpt_response(summary_prompt, child_id, is_summary=True)

        # schedule_idê°€ ìˆëŠ” ê²½ìš° treatment í…Œì´ë¸”ì— ì €ì¥
        if schedule_id:
            treatment = Schedule.query.filter_by(treatment_id=schedule_id).first()
            if treatment:
                treatment.conversation = summary
                flag_modified(treatment, "conversation")  # ë³€ê²½ ì‚¬í•­ ê°•ì œ ê°ì§€
                db.session.commit()
                logging.info(f"âœ… treatment_id {schedule_id}ì— ìš”ì•½ ì €ì¥ ì™„ë£Œ: {summary}")
            else:
                logging.warning(f"âŒ treatment_id {schedule_id}ì— í•´ë‹¹í•˜ëŠ” ì¹˜ë£Œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    socketio.emit('session_end', {'message': 'ëŒ€í™”ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'}, namespace='/')
    initialize_conversation(child_id)
    logging.info("âœ… ëŒ€í™” ì¢…ë£Œ í›„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ.")


@socketio.on('tts_finished', namespace='/')
def handle_tts_finished():
    global is_tts_playing, current_child_id, is_recognizing, keep_listening
    is_tts_playing = False
    logging.info("TTS ì¬ìƒ ì™„ë£Œ ì´ë²¤íŠ¸ ìˆ˜ì‹ : ìŒì„± ì¸ì‹ ì¬ê°œë¨")
    keep_listening = True
    Thread(target=recognize_audio, args=(current_child_id,), daemon=True).start()
