import logging
import pyaudio
import numpy as np
import whisper
import time
import openai
from threading import Lock, Thread
import os
from app.extensions import socketio

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# Whisper ëª¨ë¸ ë¡œë“œ
model = whisper.load_model("large")

# ìŒì„± ì¸ì‹ ìƒíƒœ ë³€ìˆ˜ ë° Lock
is_recognizing = False
recognition_lock = Lock()

# OpenAI API ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")


def recognize_audio():
    global is_recognizing

    with recognition_lock:
        is_recognizing = True

    CHUNK = 1024
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 16000
    RECORD_SECONDS = 0.5

    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)

    audio_buffer = []
    silence_threshold = 0.002
    silence_duration = 1.0
    last_speech_time = time.time()

    logging.info("ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘")

    while is_recognizing:
        frames = [stream.read(CHUNK, exception_on_overflow=False) for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS))]
        audio_data = b''.join(frames)
        audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0

        if np.abs(audio_np).mean() > silence_threshold:
            logging.debug("ğŸ™ ìŒì„± ê°ì§€ ì¤‘...")
            audio_buffer.append(audio_data)
            last_speech_time = time.time()
        elif time.time() - last_speech_time > silence_duration and audio_buffer:
            logging.info("ğŸ›‘ ë§ ì¤‘ë‹¨ ê°ì§€ â†’ í…ìŠ¤íŠ¸ ë³€í™˜ ì‹œë„")
            full_audio = b''.join(audio_buffer)
            audio_buffer = []

            try:
                audio_np = np.frombuffer(full_audio, dtype=np.int16).astype(np.float32) / 32768.0
                result = model.transcribe(audio_np, language="korean", temperature=0)
                text = result["text"].strip()

                if text:
                    logging.info(f"ğŸ“ í…ìŠ¤íŠ¸ ë³€í™˜ ì™„ë£Œ: {text}")
                    socketio.start_background_task(target=socketio.emit, event='recognized_text', data={'text': text})

                    # GPT ì‘ë‹µ í›„ ìŒì„± ì¸ì‹ì„ ì¬ê°œí•˜ê¸° ìœ„í•´ ìƒíƒœ ë³€ê²½
                    with recognition_lock:
                        is_recognizing = False

                    get_gpt_response(text)

            except Exception as e:
                logging.error(f"âŒ í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")

    stream.stop_stream()
    stream.close()
    p.terminate()
    logging.info("ğŸ›‘ ìŒì„± ì¸ì‹ ì¢…ë£Œ")


def get_gpt_response(user_input):
    global is_recognizing
    try:
        print(openai.api_key)
        logging.info(f"ğŸ§  GPTì— ì§ˆë¬¸: {user_input}")
        response = openai.ChatCompletion.create(
            model="gpt-4",
            prompt=user_input,
            max_tokens=150
        )
        gpt_reply = response.choices[0].text.strip()
        logging.info(f"ğŸ¤– GPT ì‘ë‹µ: {gpt_reply}")
        socketio.start_background_task(target=socketio.emit, event='gpt_response', data={'response': gpt_reply})

        # GPT ì‘ë‹µì´ ëë‚œ í›„ ìŒì„± ì¸ì‹ì„ ìƒˆ ìŠ¤ë ˆë“œë¡œ ì¬ê°œ
        with recognition_lock:
            is_recognizing = True

        Thread(target=recognize_audio, daemon=True).start()

    except Exception as e:
        logging.error(f"âŒ GPT ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")


def stop_recognition():
    global is_recognizing
    with recognition_lock:
        is_recognizing = False
    logging.info("ğŸ›‘ ìŒì„± ì¸ì‹ ì¤‘ë‹¨")
