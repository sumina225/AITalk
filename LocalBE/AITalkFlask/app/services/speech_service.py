import logging
import pyaudio
import numpy as np
import whisper
import time
import openai
from threading import Lock, Thread
import os
from gtts import gTTS
import base64
from io import BytesIO

from app.extensions import socketio, db
from app.models import Child
from dotenv import load_dotenv

load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# Whisper ëª¨ë¸ ë¡œë“œ
model = whisper.load_model("large")

# ìŒì„± ì¸ì‹ ìƒíƒœ ë³€ìˆ˜ ë° Lock
is_recognizing = False
recognition_lock = Lock()
keep_listening = True      # ìŒì„± ì¸ì‹ì„ ê³„ì†í• ì§€ ì—¬ë¶€
gpt_processing = False     # í˜„ì¬ GPT ìš”ì²­ì„ ì²˜ë¦¬ ì¤‘ì¸ì§€ ì—¬ë¶€
is_tts_playing = False     # í´ë¼ì´ì–¸íŠ¸ì—ì„œ TTS ìŒì„± ì¬ìƒ ì¤‘ì„ì„ ë‚˜íƒ€ë‚´ëŠ”  í”Œë˜ê·¸

# OpenAI API ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")

# ëŒ€í™” ë‚´ì—­ ì €ì¥ (ì´ˆê¸° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨)
conversation_history = []

def get_child_info(child_id):
    child = Child.query.filter_by(child_id=child_id).first()
    if child:
        return {
            'child_name': child.child_name,
            'child_age': child.age,
            'disability_type': child.disability_type
        }
    return None

def text_to_speech(text):
    tts = gTTS(text, lang='ko')
    audio_data = BytesIO()
    tts.write_to_fp(audio_data)
    audio_data.seek(0)
    audio_base64 = base64.b64encode(audio_data.read()).decode('utf-8')
    return audio_base64

def initialize_conversation(child_id):
    child_info = get_child_info(child_id)
    if not child_info:
        return

    child_name = child_info.get('child_name')
    child_age = child_info.get('child_age')
    child_disability_type = child_info.get('disability_type')

    initial_system_prompt = f"""
    You are a language therapy chatbot designed for a child who is {child_age} years old and has {child_disability_type}.
    Your goal is to encourage the child to speak more by asking **simple, closed-ended questions** (yes/no questions or offering simple choices), but do **not** instruct the child to answer only with "yes" or "no." Allow natural, flexible responses from the child.
    Since the child may have speech difficulties and might not pronounce words clearly, try to understand the meaning based on **context** even if the words sound incorrect. If the child's speech recognition is inaccurate, interpret it in the closest possible way to what they might have intended.
    Keep the questions simple and age-appropriate. Make sure to respond **in Korean**.
    Start with the first question: "ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì¢‹ì•„ìš”?" (Are you feeling good today?)
    """
    logging.info(f"í”„ë¡¬í”„íŠ¸ : {initial_system_prompt}")
    conversation_history.clear()
    conversation_history.append({"role": "system", "content": initial_system_prompt})

def recognize_audio(child_id):
    global is_recognizing, keep_listening, gpt_processing, is_tts_playing

    with recognition_lock:
        is_recognizing = True

    CHUNK = 1024
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 16000
    RECORD_SECONDS = 0.5

    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)

    audio_buffer = []
    silence_threshold = 0.02  # ì†ŒìŒ ì„ê³„ì¹˜
    silence_duration = 1.0     # ì¹¨ë¬µìœ¼ë¡œ ê°„ì£¼í•  ì‹œê°„
    last_speech_time = time.time()

    logging.info("ğŸ™ ìŒì„± ì¸ì‹ ì‹œì‘")

    while keep_listening:
        if is_tts_playing:
            time.sleep(0.1)
            continue

        frames = [stream.read(CHUNK, exception_on_overflow=False)
                  for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS))]
        audio_data = b''.join(frames)
        audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0

        if np.abs(audio_np).mean() > silence_threshold:
            logging.debug("ğŸ— ìŒì„± ê°ì§€ ì¤‘...")
            audio_buffer.append(audio_data)
            last_speech_time = time.time()
        elif time.time() - last_speech_time > silence_duration and audio_buffer:
            if not gpt_processing:
                logging.info("ğŸ” ë§ ì¤‘ë‹¨ ê°ì§€ â†’ í…ìŠ¤íŠ¸ ë³€í™˜ ì‹œë„")
                full_audio = b''.join(audio_buffer)
                audio_buffer = []

                try:
                    audio_np_full = np.frombuffer(full_audio, dtype=np.int16).astype(np.float32) / 32768.0
                    result = model.transcribe(audio_np_full, language="korean", temperature=0)
                    text = result["text"].strip()

                    if text:
                        logging.info(f"ğŸ“ í…ìŠ¤íŠ¸ ë³€í™˜ ì™„ë£Œ: {text}")
                        gpt_processing = True
                        Thread(target=get_gpt_response, args=(text, child_id), daemon=True).start()
                except Exception as e:
                    logging.error(f"âŒ í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
            else:
                audio_buffer = []
        time.sleep(0.01)

    stream.stop_stream()
    stream.close()
    p.terminate()
    logging.info("ğŸ” ìŒì„± ì¸ì‹ ì¢…ë£Œ")
    with recognition_lock:
        is_recognizing = False

def get_gpt_response(user_input, child_id, is_summary=False):
    global gpt_processing, conversation_history, is_tts_playing
    try:
        conversation_history.append({"role": "user", "content": user_input})
        logging.info(f"ğŸ¤ GPTì— ì§ˆë¬¸: {user_input}")
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=conversation_history,
            max_tokens=500
        )
        logging.debug(f"GPT ì›ì‹œ ì‘ë‹µ: {response}")

        gpt_reply = response.choices[0].message['content'].strip()
        logging.info(f"ğŸ¤– GPT ì‘ë‹µ: {gpt_reply}")
        conversation_history.append({"role": "assistant", "content": gpt_reply})

        if not is_summary:  # ì¼ë°˜ ëŒ€í™”ì¼ ë•Œë§Œ TTS ë³€í™˜ ë° ì „ì†¡
            is_tts_playing = True
            audio_base64 = text_to_speech(gpt_reply)
            socketio.emit('gpt_response', {'response': gpt_reply, 'audio': audio_base64}, namespace='/')
        else:  # ìš”ì•½ì¼ ê²½ìš° ì•„ë¬´ ë°ì´í„°ë„ ì „ì†¡í•˜ì§€ ì•ŠìŒ
            logging.info("âœ… ìš”ì•½ ì™„ë£Œ (í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡í•˜ì§€ ì•ŠìŒ)")

    except Exception as e:
        logging.error(f"âŒ GPT ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        gpt_processing = False

def stop_recognition(child_id):
    global keep_listening, conversation_history
    keep_listening = False
    logging.info("ğŸ” ìŒì„± ì¸ì‹ ì¤‘ë‹¨")

    child_info = get_child_info(child_id)
    if child_info:
        summary_prompt = f"""
        You are a speech therapist reviewing a conversation with a child who is {child_info['child_age']} years old and has {child_info['disability_type']}.
        Please summarize the conversation from a speech therapistâ€™s perspective in **3 concise sentences**. Focus on the childâ€™s response style, speech and pronunciation, vocabulary use, and engagement, but do not list these points separately.
        Provide the summary in **Korean**.
        """
        get_gpt_response(summary_prompt, child_id, is_summary=True)

    socketio.emit('session_end', {'message': 'ëŒ€í™”ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'}, namespace='/')

    initialize_conversation(child_id)
    logging.info("âœ… ëŒ€í™” ì¢…ë£Œ í›„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ.")

@socketio.on('tts_finished', namespace='/')
def handle_tts_finished():
    global is_tts_playing
    is_tts_playing = False
    logging.info("TTS ì¬ìƒ ì™„ë£Œ ì´ë²¤íŠ¸ ìˆ˜ì‹ : ìŒì„± ì¸ì‹ ì¬ê°œë¨")