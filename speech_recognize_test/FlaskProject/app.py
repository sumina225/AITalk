from flask import Flask, jsonify, request
from flask_socketio import SocketIO, emit
import pyaudio
import numpy as np
import whisper
import threading
import time

app = Flask(__name__)
socketio = SocketIO(app, cors_allowed_origins="*")
model = whisper.load_model("large")  # ë” ì •í™•í•œ ëª¨ë¸ ì‚¬ìš©

is_recognizing = False  # ìŒì„± ì¸ì‹ í™œì„±í™” ì—¬ë¶€


def recognize_audio():
    global is_recognizing

    CHUNK = 1024
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 16000
    RECORD_SECONDS = 0.5  # ì§§ê²Œ ì„¤ì •í•˜ì—¬ ì‹¤ì‹œê°„ ë¶„ì„

    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT,
                    channels=CHANNELS,
                    rate=RATE,
                    input=True,
                    frames_per_buffer=CHUNK)

    print("ğŸ¤ ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹œì‘...")

    audio_buffer = []  # ìŒì„± ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    silence_threshold = 0.002  # ë¬´ìŒ ê°ì§€ ê¸°ì¤€
    silence_duration = 1.0  # ì‚¬ìš©ìê°€ ë§ ì•ˆ í•  ê²½ìš° ë³€í™˜í•˜ëŠ” ì‹œê°„
    last_speech_time = time.time()

    while is_recognizing:
        frames = []
        for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):
            data = stream.read(CHUNK, exception_on_overflow=False)
            frames.append(data)

        audio_data = b''.join(frames)
        audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0

        # ğŸ”¹ ìŒì„±ì´ ìˆëŠ”ì§€ ê°ì§€ (ë„ˆë¬´ ì‘ì€ ì†Œë¦¬ëŠ” ë¬´ìŒìœ¼ë¡œ ê°„ì£¼)
        if np.abs(audio_np).mean() > silence_threshold:
            print("ğŸ™ ìŒì„± ê°ì§€ë¨, ë…¹ìŒ ì¤‘...")
            audio_buffer.append(audio_data)
            last_speech_time = time.time()  # ë§ˆì§€ë§‰ìœ¼ë¡œ ë§ì„ í•œ ì‹œì  ì €ì¥
        else:
            # ì‚¬ìš©ìê°€ 1ì´ˆ ì´ìƒ ë§í•˜ì§€ ì•Šìœ¼ë©´ ë³€í™˜
            if time.time() - last_speech_time > silence_duration and audio_buffer:
                print("ğŸ›‘ ë§ ëë‚¨ â†’ í…ìŠ¤íŠ¸ ë³€í™˜ ì‹œì‘...")

                full_audio = b''.join(audio_buffer)
                audio_buffer = []  # ë²„í¼ ì´ˆê¸°í™”

                # ìŒì„± ë°ì´í„°ë¥¼ Whisperë¡œ ë³€í™˜
                try:
                    audio_np = np.frombuffer(full_audio, dtype=np.int16).astype(np.float32) / 32768.0
                    result = model.transcribe(audio_np, language="korean", temperature=0)  # ë” ì •í™•í•œ ë³€í™˜
                    text = result["text"].strip()

                    if text:
                        print(f"ğŸ“ ë³€í™˜ëœ í…ìŠ¤íŠ¸: {text}")
                        socketio.emit('recognized_text', {'text': text})  # Reactë¡œ ì „ì†¡
                except Exception as e:
                    print("âŒ ì˜¤ë¥˜ ë°œìƒ:", str(e))

    stream.stop_stream()
    stream.close()
    p.terminate()
    print("ğŸ›‘ ìŒì„± ì¸ì‹ ì¢…ë£Œ")


@app.route('/start-recognition', methods=['POST'])
def start_recognition():
    global is_recognizing

    if is_recognizing:
        return jsonify({"status": "already running"})

    is_recognizing = True
    threading.Thread(target=recognize_audio, daemon=True).start()
    return jsonify({"status": "started"})


@app.route('/stop-recognition', methods=['POST'])
def stop_recognition():
    global is_recognizing
    is_recognizing = False
    return jsonify({"status": "stopped"})


if __name__ == '__main__':
    socketio.run(app, host="localhost", port=5000, debug=True)
